general:
  exp_name: 0922_X
  logdir: logs
  demodir: demos
  seed: random

hardware:
  gpu: [0,1,2] 
  cpu_min: [1,1,1]
  cpu_max: [32,32,32]

environment:
  name: input
  task: 
    name: input
    params: []
  headless: True
  obs_dim: auto
  act_dim: auto
  camera: False
  reward:
    reward_scalor: 1
    reward_bonus: 0.1
    reward_shaping_type: sparse

agent:
  type: input
  hidden_sizes: [256,256,256]
  boundary_min: auto
  boundary_max: auto
  gamma: 0.95
  polyak: 0.995
  learning_rate: 0.001
  sac:
    alpha: 0.2 # spinningup do not have "auto" option
  td3:
    target_noise: 0.2
    noise_clip: 0.5
    policy_delay : 2
    act_noise: 0.1

buffer:
  replay_buffer_size: 1e6
  her:
    goal_selection_strategy: input # in SB3 only the orginal ones are implemented
    n_sampled_goal: 4
    env_check: true

sampler:
  start_steps: 10000 # in SB3 we cannot set it
  max_ep_len: auto

trainer:
  mode: input
  total_timesteps: 5e5
  batch_size: 256
  update_after: 1e2
  update_every: 50
  cl:
    type: input # predefined / selfpaced / controldiscrete / selfpaceddual
    predefined:
      pacing_profile: linear  # linear / sqrt / quad
      pacing_sat: 0.8 
    selfpaced:
      conv_cond: 0.9
      window_size: 10
      step: 0.05
    selfpaceddual:
      upper_cond: 0.9
      lower_cond: 0.1
      window_size: 10
      step: 0.05
    controldiscrete:
      target: 0.8
      step: 0.01
      window_size: 10
    # controlpd:
    #   target_profile: linear
    #   target_sat: 0.75
    #   p: 0.01
    #   d: 0

eval:
  freq: 1e4
  num_episodes: 20
  

logger:
  rollout:
    stats_window_size: 10
  train:
    stats_window_size: 1
  model:
    save:
      mode: pi
      best_start_t: 0.0
      freq: 20
