general:
  exp_name: 1020_X
  logdir: logs
  demodir: demos
  seed: random
  init_weights:
    bool: False
    mode: all # all / pi
    path: logs/0928_X_PandaPush-v3_sac_controldiscrete_const/0/model_backup/model_best_model

hardware:
  gpu: [0,1,2,3] 
  cpu_min: [1,1,1,1]
  cpu_max: [32,32,32,32]

environment:
  name: input # gympanda
  task: 
    name: input # PandaReach-v3 / PandaPush-v3 / PandaSlide-v3 (and dense versions)
    params: []
  headless: True
  obs_dim: auto
  act_dim: auto
  camera: False
  reward:
    reward_scalor: 1
    reward_bonus: input
    reward_shaping_type: input # sparse / state_change_bonus

agent:
  type: input
  hidden_sizes: [256,256,256]
  boundary_min: auto
  boundary_max: auto
  gamma: 0.95
  polyak: 0.995
  learning_rate: 0.001
  sac:
    alpha: 0.2 # spinningup do not have "auto" option
  td3:
    target_noise: 0.2
    noise_clip: 0.5
    policy_delay : 2
    act_noise: 0.1
  ddpg:
    act_noise: 0.1

buffer:
  replay_buffer_size: input
  her:
    goal_selection_strategy: input # noher / final / future / future_once / near / next
    n_sampled_goal: 4
    state_check: true
  per:
    mode: input # noper / proportional
    alpha: 0.6
    beta_start: 0.4
    beta_frames_ratio: 0.8
  highlights:
    mode: input # nohl / fix / ama / predefined / multifix / multipredefined
    batch_ratio_mode: input # fix / prioritized (works only if 'mode' is not 'nohl')
    batch_ratio_prioritized_alpha: 0.95
    batch_ratio: input
    batch_ratio_min: 0.1 # in case of prioritized
    batch_ratio_max: 0.9 # in case of prioritized
    buffer_size: 5e4
    success_cond: False
    include_test: True
    fix:
      threshold: -20 
    ama: # adaptive moving average
      threshold_start: -50
      threshold_margin: 10
      window: 20
    predefined:
      threshold_pacing_profile: linear  # linear / sqrt / quad
      threshold_pacing_sat: 0.8
      threshold_start: -50
      threshold_end: -10
    multifix:
      thresholds: [-10,-20,-30,-40]
      batch_ratios: [0.125, 0.125, 0.125, 0.125]
      batch_bin_min_sample: 10



sampler:
  start_steps: 10000 # in SB3 we cannot set it
  max_ep_len: auto

trainer:
  total_timesteps: input
  batch_size: 256
  update_after: 1e2
  update_every: 50
  cl:
    type: input # nocl / nullcl / predefined / predefinedtwostage / predefinedthreestage / selfpaced / selfpaceddual / controldiscrete / controldiscreteadaptive
    range_growth_mode: input # simple / discard / balancediscard
    ratio_discard_lag: 0.1 # Only if 'range_growth_mode' is 'discard' or 'balancediscard'
    balancediscard_ratio: 0.8  # Only if 'range_growth_mode' is 'balancediscard'
    predefined:
      pacing_profile: input  # linear / sqrt / quad
      pacing_sat: 0.8
    predefinedtwostage:
      pacing_profile: input  # linear / sqrt / quad
      change_stage: 0.5
      stage1: # goal
        pacing_sat: 0.4
      stage2: # obj
        pacing_sat: 0.8
    predefinedthreestage:
      pacing_profile: input  # linear / sqrt / quad
      change_stage12: 0.3
      change_stage23: 0.6
      stage1: # goal
        pacing_sat: 0.2
      stage2: # obj
        pacing_sat: 0.5
      stage3: # obj-goal
        pacing_sat: 0.8
    selfpaced:
      conv_cond: 0.8
      window_size: 20
      step: 0.05
    selfpaceddual:
      upper_cond: 0.8
      lower_cond: 0.2
      window_size: 20
      step: 0.05
    controldiscrete:
      target_profile: input # const / linear / sqrt / quad
      target_sat: 0.8 
      target_sat_value: 0.8
      step: 0.01
      window_size: 20
      const_sin:
        amp: 0.05
        freq_divide: 50
    controldiscreteadaptive:
      target_plus_eval: 0.2
      target_max: 0.9
      window_size_eval: 3
      window_size_rollout: 20
      step: 0.01

eval:
  freq: input
  num_episodes: input
  

logger:
  rollout:
    stats_window_size: 10
  train:
    stats_window_size: 1
  state_change:
    stats_window_size: 100
  model:
    save:
      mode: pi
      best_start_t: 0.0
      freq: 20
      measure: reward # reward / success_rate