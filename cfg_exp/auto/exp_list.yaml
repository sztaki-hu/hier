process_0: # noher - nohl - noper - nocl
  general:
    seednum: 10 # 3 (tasks) x 10(seeds) x 3h(runtime) = 90h = 3.75 days 
  agent: 
    type: ['sac'] # sac / td3 / ddpg
    sac:
      alpha: [0.1]
    gamma: [0.95]
    learning_rate: [0.01]
  environment:
    reward:
      reward_shaping_type: ['sparse'] # sparse / state_change_bonus
      reward_bonus: [0.0] #--- only if reward_shaping_type == state_change_bonus
  buffer:
    replay_buffer_size: [1e6]
    her:
      goal_selection_strategy: ['noher'] # noher / final / future / future_once / near / next
    highlights:
      mode: ['nohl'] # nohl / fix / ama / amar / predefined / multifix / multipredefined
      batch_ratio_mode: ['fix'] # fix / prioritized (works only if 'mode' is not 'nohl')
      batch_ratio: [0.5] #--- only if highlights mode != nohl
      buffer_size: [1e6]
      fix: #--- only if highlights mode == fix
        threshold: [-20] # gympanda: -20 / gym: [Hopper: 2500 / HalfCheetah: 7000 / Walker2d: 3000 / Ant: 3000]
      predefined: #--- only if highlights mode == predefined
        threshold_start: [-50] # gympanda: -50 / gym: 0
        threshold_end: [-10] # gympanda: -10 / gym: [Hopper: 3000 / HalfCheetah: 8000 / Walker2d: 3500 / Ant: 4000]
    per:
      mode: ['noper'] # noper / proportional
  trainer:
    total_timesteps: [5e5]
  eval:
    freq: [1e4]
    num_episodes: [100]
  # structure is differend than in the config file ##################################################x
  task: 
    #gym: ['Hopper-v4'] # 'Hopper-v4','HalfCheetah-v4','Walker2d-v4','Ant-v4'
    gympanda: ['PandaPush-v3','PandaSlide-v3','PandaPickAndPlace-v3'] # PandaReach-v3 / PandaPush-v3 / PandaSlide-v3 / PandaPickAndPlace-v3 / PandaStack-v3
  cl: 
    type: ['nocl'] # nocl / nullcl / predefined / predefinedtwostage / predefinedthreestage / selfpaced / selfpaceddual / controldiscrete / controldiscreteadaptive
    range_growth_mode: ['simple'] #--- only if highlights mode != nocl
####################################################################################################

process_1: # her - nohl - noper - nocl
  general:
    seednum: 10 # 3 (tasks) x 10(seeds) x 3h(runtime) = 90h = 3.75 days 
  agent: 
    type: ['sac'] # sac / td3 / ddpg
    sac:
      alpha: [0.1]
    gamma: [0.95]
    learning_rate: [0.01]
  environment:
    reward:
      reward_shaping_type: ['sparse'] # sparse / state_change_bonus
      reward_bonus: [0.0] #--- only if reward_shaping_type == state_change_bonus
  buffer:
    replay_buffer_size: [1e6]
    her:
      goal_selection_strategy: ['final'] # noher / final / future / future_once / near / next
    highlights:
      mode: ['nohl'] # nohl / fix / ama / amar / predefined / multifix / multipredefined
      batch_ratio_mode: ['fix'] # fix / prioritized (works only if 'mode' is not 'nohl')
      batch_ratio: [0.5] #--- only if highlights mode != nohl
      buffer_size: [1e6]
      fix: #--- only if highlights mode == fix
        threshold: [-20] # gympanda: -20 / gym: [Hopper: 2500 / HalfCheetah: 7000 / Walker2d: 3000 / Ant: 3000]
      predefined: #--- only if highlights mode == predefined
        threshold_start: [-50] # gympanda: -50 / gym: 0
        threshold_end: [-10] # gympanda: -10 / gym: [Hopper: 3000 / HalfCheetah: 8000 / Walker2d: 3500 / Ant: 4000]
    per:
      mode: ['noper'] # noper / proportional
  trainer:
    total_timesteps: [5e5]
  eval:
    freq: [1e4]
    num_episodes: [100]
  # structure is differend than in the config file ##################################################x
  task: 
    #gym: ['Hopper-v4'] # 'Hopper-v4','HalfCheetah-v4','Walker2d-v4','Ant-v4'
    gympanda: ['PandaPush-v3','PandaSlide-v3','PandaPickAndPlace-v3'] # PandaReach-v3 / PandaPush-v3 / PandaSlide-v3 / PandaPickAndPlace-v3 / PandaStack-v3
  cl: 
    type: ['nocl'] # nocl / nullcl / predefined / predefinedtwostage / predefinedthreestage / selfpaced / selfpaceddual / controldiscrete / controldiscreteadaptive
    range_growth_mode: ['simple'] #--- only if highlights mode != nocl
####################################################################################################

process_2: # noher - nohl - per - nocl
  general:
    seednum: 10 # 3 (tasks) x 10(seeds) x 3h(runtime) = 90h = 3.75 days 
  agent: 
    type: ['sac'] # sac / td3 / ddpg
    sac:
      alpha: [0.1]
    gamma: [0.95]
    learning_rate: [0.01]
  environment:
    reward:
      reward_shaping_type: ['sparse'] # sparse / state_change_bonus
      reward_bonus: [0.0] #--- only if reward_shaping_type == state_change_bonus
  buffer:
    replay_buffer_size: [1e6]
    her:
      goal_selection_strategy: ['noher'] # noher / final / future / future_once / near / next
    highlights:
      mode: ['nohl'] # nohl / fix / ama / amar / predefined / multifix / multipredefined
      batch_ratio_mode: ['fix'] # fix / prioritized (works only if 'mode' is not 'nohl')
      batch_ratio: [0.5] #--- only if highlights mode != nohl
      buffer_size: [1e6]
      fix: #--- only if highlights mode == fix
        threshold: [-20] # gympanda: -20 / gym: [Hopper: 2500 / HalfCheetah: 7000 / Walker2d: 3000 / Ant: 3000]
      predefined: #--- only if highlights mode == predefined
        threshold_start: [-50] # gympanda: -50 / gym: 0
        threshold_end: [-10] # gympanda: -10 / gym: [Hopper: 3000 / HalfCheetah: 8000 / Walker2d: 3500 / Ant: 4000]
    per:
      mode: ['proportional'] # noper / proportional
  trainer:
    total_timesteps: [5e5]
  eval:
    freq: [1e4]
    num_episodes: [100]
  # structure is differend than in the config file ##################################################x
  task: 
    #gym: ['Hopper-v4'] # 'Hopper-v4','HalfCheetah-v4','Walker2d-v4','Ant-v4'
    gympanda: ['PandaPush-v3','PandaSlide-v3','PandaPickAndPlace-v3'] # PandaReach-v3 / PandaPush-v3 / PandaSlide-v3 / PandaPickAndPlace-v3 / PandaStack-v3
  cl: 
    type: ['nocl'] # nocl / nullcl / predefined / predefinedtwostage / predefinedthreestage / selfpaced / selfpaceddual / controldiscrete / controldiscreteadaptive
    range_growth_mode: ['simple'] #--- only if highlights mode != nocl
####################################################################################################

process_3: # noher - nohl - noper - cl
  general:
    seednum: 10 # 3 (tasks) x 10(seeds) x 3h(runtime) = 90h = 3.75 days 
  agent: 
    type: ['sac'] # sac / td3 / ddpg
    sac:
      alpha: [0.1]
    gamma: [0.95]
    learning_rate: [0.01]
  environment:
    reward:
      reward_shaping_type: ['sparse'] # sparse / state_change_bonus
      reward_bonus: [0.0] #--- only if reward_shaping_type == state_change_bonus
  buffer:
    replay_buffer_size: [1e6]
    her:
      goal_selection_strategy: ['noher'] # noher / final / future / future_once / near / next
    highlights:
      mode: ['nohl'] # nohl / fix / ama / amar / predefined / multifix / multipredefined
      batch_ratio_mode: ['fix'] # fix / prioritized (works only if 'mode' is not 'nohl')
      batch_ratio: [0.5] #--- only if highlights mode != nohl
      buffer_size: [1e6]
      fix: #--- only if highlights mode == fix
        threshold: [-20] # gympanda: -20 / gym: [Hopper: 2500 / HalfCheetah: 7000 / Walker2d: 3000 / Ant: 3000]
      predefined: #--- only if highlights mode == predefined
        threshold_start: [-50] # gympanda: -50 / gym: 0
        threshold_end: [-10] # gympanda: -10 / gym: [Hopper: 3000 / HalfCheetah: 8000 / Walker2d: 3500 / Ant: 4000]
    per:
      mode: ['noper'] # noper / proportional
  trainer:
    total_timesteps: [5e5]
  eval:
    freq: [1e4]
    num_episodes: [100]
  # structure is differend than in the config file ##################################################x
  task: 
    #gym: ['Hopper-v4'] # 'Hopper-v4','HalfCheetah-v4','Walker2d-v4','Ant-v4'
    gympanda: ['PandaPush-v3','PandaSlide-v3','PandaPickAndPlace-v3'] # PandaReach-v3 / PandaPush-v3 / PandaSlide-v3 / PandaPickAndPlace-v3 / PandaStack-v3
  cl: 
    type: ['controldiscreteadaptive'] # nocl / nullcl / predefined / predefinedtwostage / predefinedthreestage / selfpaced / selfpaceddual / controldiscrete / controldiscreteadaptive
    range_growth_mode: ['simple'] #--- only if highlights mode != nocl
####################################################################################################

process_4: # her - nohl - noper - cl
  general:
    seednum: 10 # 3 (tasks) x 10(seeds) x 3h(runtime) = 90h = 3.75 days 
  agent: 
    type: ['sac'] # sac / td3 / ddpg
    sac:
      alpha: [0.1]
    gamma: [0.95]
    learning_rate: [0.01]
  environment:
    reward:
      reward_shaping_type: ['sparse'] # sparse / state_change_bonus
      reward_bonus: [0.0] #--- only if reward_shaping_type == state_change_bonus
  buffer:
    replay_buffer_size: [1e6]
    her:
      goal_selection_strategy: ['final'] # noher / final / future / future_once / near / next
    highlights:
      mode: ['nohl'] # nohl / fix / ama / amar / predefined / multifix / multipredefined
      batch_ratio_mode: ['fix'] # fix / prioritized (works only if 'mode' is not 'nohl')
      batch_ratio: [0.5] #--- only if highlights mode != nohl
      buffer_size: [1e6]
      fix: #--- only if highlights mode == fix
        threshold: [-20] # gympanda: -20 / gym: [Hopper: 2500 / HalfCheetah: 7000 / Walker2d: 3000 / Ant: 3000]
      predefined: #--- only if highlights mode == predefined
        threshold_start: [-50] # gympanda: -50 / gym: 0
        threshold_end: [-10] # gympanda: -10 / gym: [Hopper: 3000 / HalfCheetah: 8000 / Walker2d: 3500 / Ant: 4000]
    per:
      mode: ['noper'] # noper / proportional
  trainer:
    total_timesteps: [5e5]
  eval:
    freq: [1e4]
    num_episodes: [100]
  # structure is differend than in the config file ##################################################x
  task: 
    #gym: ['Hopper-v4'] # 'Hopper-v4','HalfCheetah-v4','Walker2d-v4','Ant-v4'
    gympanda: ['PandaPush-v3','PandaSlide-v3','PandaPickAndPlace-v3'] # PandaReach-v3 / PandaPush-v3 / PandaSlide-v3 / PandaPickAndPlace-v3 / PandaStack-v3
  cl: 
    type: ['controldiscreteadaptive'] # nocl / nullcl / predefined / predefinedtwostage / predefinedthreestage / selfpaced / selfpaceddual / controldiscrete / controldiscreteadaptive
    range_growth_mode: ['simple'] #--- only if highlights mode != nocl
####################################################################################################

process_5: # noher - nohl - per - cl
  general:
    seednum: 10 # 3 (tasks) x 10(seeds) x 3h(runtime) = 90h = 3.75 days 
  agent: 
    type: ['sac'] # sac / td3 / ddpg
    sac:
      alpha: [0.1]
    gamma: [0.95]
    learning_rate: [0.01]
  environment:
    reward:
      reward_shaping_type: ['sparse'] # sparse / state_change_bonus
      reward_bonus: [0.0] #--- only if reward_shaping_type == state_change_bonus
  buffer:
    replay_buffer_size: [1e6]
    her:
      goal_selection_strategy: ['noher'] # noher / final / future / future_once / near / next
    highlights:
      mode: ['nohl'] # nohl / fix / ama / amar / predefined / multifix / multipredefined
      batch_ratio_mode: ['fix'] # fix / prioritized (works only if 'mode' is not 'nohl')
      batch_ratio: [0.5] #--- only if highlights mode != nohl
      buffer_size: [1e6]
      fix: #--- only if highlights mode == fix
        threshold: [-20] # gympanda: -20 / gym: [Hopper: 2500 / HalfCheetah: 7000 / Walker2d: 3000 / Ant: 3000]
      predefined: #--- only if highlights mode == predefined
        threshold_start: [-50] # gympanda: -50 / gym: 0
        threshold_end: [-10] # gympanda: -10 / gym: [Hopper: 3000 / HalfCheetah: 8000 / Walker2d: 3500 / Ant: 4000]
    per:
      mode: ['proportional'] # noper / proportional
  trainer:
    total_timesteps: [5e5]
  eval:
    freq: [1e4]
    num_episodes: [100]
  # structure is differend than in the config file ##################################################x
  task: 
    #gym: ['Hopper-v4'] # 'Hopper-v4','HalfCheetah-v4','Walker2d-v4','Ant-v4'
    gympanda: ['PandaPush-v3','PandaSlide-v3','PandaPickAndPlace-v3'] # PandaReach-v3 / PandaPush-v3 / PandaSlide-v3 / PandaPickAndPlace-v3 / PandaStack-v3
  cl: 
    type: ['controldiscreteadaptive'] # nocl / nullcl / predefined / predefinedtwostage / predefinedthreestage / selfpaced / selfpaceddual / controldiscrete / controldiscreteadaptive
    range_growth_mode: ['simple'] #--- only if highlights mode != nocl
####################################################################################################

process_6: # noher - hl - noper - nocl
  general:
    seednum: 10 # 3 (tasks) x 10(seeds) x 3h(runtime) = 90h = 3.75 days 
  agent: 
    type: ['sac'] # sac / td3 / ddpg
    sac:
      alpha: [0.1]
    gamma: [0.95]
    learning_rate: [0.01]
  environment:
    reward:
      reward_shaping_type: ['sparse'] # sparse / state_change_bonus
      reward_bonus: [0.0] #--- only if reward_shaping_type == state_change_bonus
  buffer:
    replay_buffer_size: [1e6]
    her:
      goal_selection_strategy: ['noher'] # noher / final / future / future_once / near / next
    highlights:
      mode: ['predefined'] # nohl / fix / ama / amar / predefined / multifix / multipredefined
      batch_ratio_mode: ['fix'] # fix / prioritized (works only if 'mode' is not 'nohl')
      batch_ratio: [0.5] #--- only if highlights mode != nohl
      buffer_size: [1e6]
      fix: #--- only if highlights mode == fix
        threshold: [-20] # gympanda: -20 / gym: [Hopper: 2500 / HalfCheetah: 7000 / Walker2d: 3000 / Ant: 3000]
      predefined: #--- only if highlights mode == predefined
        threshold_start: [-50] # gympanda: -50 / gym: 0
        threshold_end: [-10] # gympanda: -10 / gym: [Hopper: 3000 / HalfCheetah: 8000 / Walker2d: 3500 / Ant: 4000]
    per:
      mode: ['noper'] # noper / proportional
  trainer:
    total_timesteps: [5e5]
  eval:
    freq: [1e4]
    num_episodes: [100]
  # structure is differend than in the config file ##################################################x
  task: 
    #gym: ['Hopper-v4'] # 'Hopper-v4','HalfCheetah-v4','Walker2d-v4','Ant-v4'
    gympanda: ['PandaPush-v3','PandaSlide-v3','PandaPickAndPlace-v3'] # PandaReach-v3 / PandaPush-v3 / PandaSlide-v3 / PandaPickAndPlace-v3 / PandaStack-v3
  cl: 
    type: ['nocl'] # nocl / nullcl / predefined / predefinedtwostage / predefinedthreestage / selfpaced / selfpaceddual / controldiscrete / controldiscreteadaptive
    range_growth_mode: ['simple'] #--- only if highlights mode != nocl
####################################################################################################

process_7: # her - hl - noper - nocl
  general:
    seednum: 10 # 3 (tasks) x 10(seeds) x 3h(runtime) = 90h = 3.75 days 
  agent: 
    type: ['sac'] # sac / td3 / ddpg
    sac:
      alpha: [0.1]
    gamma: [0.95]
    learning_rate: [0.01]
  environment:
    reward:
      reward_shaping_type: ['sparse'] # sparse / state_change_bonus
      reward_bonus: [0.0] #--- only if reward_shaping_type == state_change_bonus
  buffer:
    replay_buffer_size: [1e6]
    her:
      goal_selection_strategy: ['final'] # noher / final / future / future_once / near / next
    highlights:
      mode: ['predefined'] # nohl / fix / ama / amar / predefined / multifix / multipredefined
      batch_ratio_mode: ['fix'] # fix / prioritized (works only if 'mode' is not 'nohl')
      batch_ratio: [0.5] #--- only if highlights mode != nohl
      buffer_size: [1e6]
      fix: #--- only if highlights mode == fix
        threshold: [-20] # gympanda: -20 / gym: [Hopper: 2500 / HalfCheetah: 7000 / Walker2d: 3000 / Ant: 3000]
      predefined: #--- only if highlights mode == predefined
        threshold_start: [-50] # gympanda: -50 / gym: 0
        threshold_end: [-10] # gympanda: -10 / gym: [Hopper: 3000 / HalfCheetah: 8000 / Walker2d: 3500 / Ant: 4000]
    per:
      mode: ['noper'] # noper / proportional
  trainer:
    total_timesteps: [5e5]
  eval:
    freq: [1e4]
    num_episodes: [100]
  # structure is differend than in the config file ##################################################x
  task: 
    #gym: ['Hopper-v4'] # 'Hopper-v4','HalfCheetah-v4','Walker2d-v4','Ant-v4'
    gympanda: ['PandaPush-v3','PandaSlide-v3','PandaPickAndPlace-v3'] # PandaReach-v3 / PandaPush-v3 / PandaSlide-v3 / PandaPickAndPlace-v3 / PandaStack-v3
  cl: 
    type: ['nocl'] # nocl / nullcl / predefined / predefinedtwostage / predefinedthreestage / selfpaced / selfpaceddual / controldiscrete / controldiscreteadaptive
    range_growth_mode: ['simple'] #--- only if highlights mode != nocl
####################################################################################################

process_8: # noher - hl - noper - cl
  general:
    seednum: 10 # 3 (tasks) x 10(seeds) x 3h(runtime) = 90h = 3.75 days 
  agent: 
    type: ['sac'] # sac / td3 / ddpg
    sac:
      alpha: [0.1]
    gamma: [0.95]
    learning_rate: [0.01]
  environment:
    reward:
      reward_shaping_type: ['sparse'] # sparse / state_change_bonus
      reward_bonus: [0.0] #--- only if reward_shaping_type == state_change_bonus
  buffer:
    replay_buffer_size: [1e6]
    her:
      goal_selection_strategy: ['noher'] # noher / final / future / future_once / near / next
    highlights:
      mode: ['predefined'] # nohl / fix / ama / amar / predefined / multifix / multipredefined
      batch_ratio_mode: ['fix'] # fix / prioritized (works only if 'mode' is not 'nohl')
      batch_ratio: [0.5] #--- only if highlights mode != nohl
      buffer_size: [1e6]
      fix: #--- only if highlights mode == fix
        threshold: [-20] # gympanda: -20 / gym: [Hopper: 2500 / HalfCheetah: 7000 / Walker2d: 3000 / Ant: 3000]
      predefined: #--- only if highlights mode == predefined
        threshold_start: [-50] # gympanda: -50 / gym: 0
        threshold_end: [-10] # gympanda: -10 / gym: [Hopper: 3000 / HalfCheetah: 8000 / Walker2d: 3500 / Ant: 4000]
    per:
      mode: ['noper'] # noper / proportional
  trainer:
    total_timesteps: [5e5]
  eval:
    freq: [1e4]
    num_episodes: [100]
  # structure is differend than in the config file ##################################################x
  task: 
    #gym: ['Hopper-v4'] # 'Hopper-v4','HalfCheetah-v4','Walker2d-v4','Ant-v4'
    gympanda: ['PandaPush-v3','PandaSlide-v3','PandaPickAndPlace-v3'] # PandaReach-v3 / PandaPush-v3 / PandaSlide-v3 / PandaPickAndPlace-v3 / PandaStack-v3
  cl: 
    type: ['controldiscreteadaptive'] # nocl / nullcl / predefined / predefinedtwostage / predefinedthreestage / selfpaced / selfpaceddual / controldiscrete / controldiscreteadaptive
    range_growth_mode: ['simple'] #--- only if highlights mode != nocl
####################################################################################################

process_9: # her - hl - noper - cl
  general:
    seednum: 10 # 3 (tasks) x 10(seeds) x 3h(runtime) = 90h = 3.75 days 
  agent: 
    type: ['sac'] # sac / td3 / ddpg
    sac:
      alpha: [0.1]
    gamma: [0.95]
    learning_rate: [0.01]
  environment:
    reward:
      reward_shaping_type: ['sparse'] # sparse / state_change_bonus
      reward_bonus: [0.0] #--- only if reward_shaping_type == state_change_bonus
  buffer:
    replay_buffer_size: [1e6]
    her:
      goal_selection_strategy: ['final'] # noher / final / future / future_once / near / next
    highlights:
      mode: ['predefined'] # nohl / fix / ama / amar / predefined / multifix / multipredefined
      batch_ratio_mode: ['fix'] # fix / prioritized (works only if 'mode' is not 'nohl')
      batch_ratio: [0.5] #--- only if highlights mode != nohl
      buffer_size: [1e6]
      fix: #--- only if highlights mode == fix
        threshold: [-20] # gympanda: -20 / gym: [Hopper: 2500 / HalfCheetah: 7000 / Walker2d: 3000 / Ant: 3000]
      predefined: #--- only if highlights mode == predefined
        threshold_start: [-50] # gympanda: -50 / gym: 0
        threshold_end: [-10] # gympanda: -10 / gym: [Hopper: 3000 / HalfCheetah: 8000 / Walker2d: 3500 / Ant: 4000]
    per:
      mode: ['noper'] # noper / proportional
  trainer:
    total_timesteps: [5e5]
  eval:
    freq: [1e4]
    num_episodes: [100]
  # structure is differend than in the config file ##################################################x
  task: 
    #gym: ['Hopper-v4'] # 'Hopper-v4','HalfCheetah-v4','Walker2d-v4','Ant-v4'
    gympanda: ['PandaPush-v3','PandaSlide-v3','PandaPickAndPlace-v3'] # PandaReach-v3 / PandaPush-v3 / PandaSlide-v3 / PandaPickAndPlace-v3 / PandaStack-v3
  cl: 
    type: ['controldiscreteadaptive'] # nocl / nullcl / predefined / predefinedtwostage / predefinedthreestage / selfpaced / selfpaceddual / controldiscrete / controldiscreteadaptive
    range_growth_mode: ['simple'] #--- only if highlights mode != nocl
####################################################################################################

process_10: # noher - hl - per - cl
  general:
    seednum: 10 # 3 (tasks) x 10(seeds) x 3h(runtime) = 90h = 3.75 days 
  agent: 
    type: ['sac'] # sac / td3 / ddpg
    sac:
      alpha: [0.1]
    gamma: [0.95]
    learning_rate: [0.01]
  environment:
    reward:
      reward_shaping_type: ['sparse'] # sparse / state_change_bonus
      reward_bonus: [0.0] #--- only if reward_shaping_type == state_change_bonus
  buffer:
    replay_buffer_size: [1e6]
    her:
      goal_selection_strategy: ['noher'] # noher / final / future / future_once / near / next
    highlights:
      mode: ['predefined'] # nohl / fix / ama / amar / predefined / multifix / multipredefined
      batch_ratio_mode: ['fix'] # fix / prioritized (works only if 'mode' is not 'nohl')
      batch_ratio: [0.5] #--- only if highlights mode != nohl
      buffer_size: [1e6]
      fix: #--- only if highlights mode == fix
        threshold: [-20] # gympanda: -20 / gym: [Hopper: 2500 / HalfCheetah: 7000 / Walker2d: 3000 / Ant: 3000]
      predefined: #--- only if highlights mode == predefined
        threshold_start: [-50] # gympanda: -50 / gym: 0
        threshold_end: [-10] # gympanda: -10 / gym: [Hopper: 3000 / HalfCheetah: 8000 / Walker2d: 3500 / Ant: 4000]
    per:
      mode: ['proportional'] # noper / proportional
  trainer:
    total_timesteps: [5e5]
  eval:
    freq: [1e4]
    num_episodes: [100]
  # structure is differend than in the config file ##################################################x
  task: 
    #gym: ['Hopper-v4'] # 'Hopper-v4','HalfCheetah-v4','Walker2d-v4','Ant-v4'
    gympanda: ['PandaPush-v3','PandaSlide-v3','PandaPickAndPlace-v3'] # PandaReach-v3 / PandaPush-v3 / PandaSlide-v3 / PandaPickAndPlace-v3 / PandaStack-v3
  cl: 
    type: ['controldiscreteadaptive'] # nocl / nullcl / predefined / predefinedtwostage / predefinedthreestage / selfpaced / selfpaceddual / controldiscrete / controldiscreteadaptive
    range_growth_mode: ['simple'] #--- only if highlights mode != nocl
####################################################################################################

process_11: # her - hl - per - cl
  general:
    seednum: 10 # 3 (tasks) x 10(seeds) x 3h(runtime) = 90h = 3.75 days 
  agent: 
    type: ['sac'] # sac / td3 / ddpg
    sac:
      alpha: [0.1]
    gamma: [0.95]
    learning_rate: [0.01]
  environment:
    reward:
      reward_shaping_type: ['sparse'] # sparse / state_change_bonus
      reward_bonus: [0.0] #--- only if reward_shaping_type == state_change_bonus
  buffer:
    replay_buffer_size: [1e6]
    her:
      goal_selection_strategy: ['final'] # noher / final / future / future_once / near / next
    highlights:
      mode: ['predefined'] # nohl / fix / ama / amar / predefined / multifix / multipredefined
      batch_ratio_mode: ['fix'] # fix / prioritized (works only if 'mode' is not 'nohl')
      batch_ratio: [0.5] #--- only if highlights mode != nohl
      buffer_size: [1e6]
      fix: #--- only if highlights mode == fix
        threshold: [-20] # gympanda: -20 / gym: [Hopper: 2500 / HalfCheetah: 7000 / Walker2d: 3000 / Ant: 3000]
      predefined: #--- only if highlights mode == predefined
        threshold_start: [-50] # gympanda: -50 / gym: 0
        threshold_end: [-10] # gympanda: -10 / gym: [Hopper: 3000 / HalfCheetah: 8000 / Walker2d: 3500 / Ant: 4000]
    per:
      mode: ['proportional'] # noper / proportional
  trainer:
    total_timesteps: [5e5]
  eval:
    freq: [1e4]
    num_episodes: [100]
  # structure is differend than in the config file ##################################################x
  task: 
    #gym: ['Hopper-v4'] # 'Hopper-v4','HalfCheetah-v4','Walker2d-v4','Ant-v4'
    gympanda: ['PandaPush-v3','PandaSlide-v3','PandaPickAndPlace-v3'] # PandaReach-v3 / PandaPush-v3 / PandaSlide-v3 / PandaPickAndPlace-v3 / PandaStack-v3
  cl: 
    type: ['controldiscreteadaptive'] # nocl / nullcl / predefined / predefinedtwostage / predefinedthreestage / selfpaced / selfpaceddual / controldiscrete / controldiscreteadaptive
    range_growth_mode: ['simple'] #--- only if highlights mode != nocl
####################################################################################################