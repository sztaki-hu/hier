process_0:
  agent: 
    type: ['sac','td3','ddpg']
  environment:
    reward:
      reward_shaping_type: ['sparse'] # sparse / state_change_bonus
      reward_bonus: [0.0]
  buffer:
    replay_buffer_size: [1e6]
    her:
      goal_selection_strategy: ['noher','final'] # noher / final / future / future_once / near / next
    highlights:
      mode: ['nohl'] # nohl / fix_th / ama
      batch_ratio: [0.0]
    per:
      mode: ['noper'] # noper / proportional
  trainer:
    total_timesteps: [5e5]
  eval:
    freq: [1e4]
  # structure is differend than in the config file ##################################################x
  task: 
    gympanda: ['PandaPush-v3'] # PandaReach-v3 / PandaPush-v3 / PandaSlide-v3 / PandaPickAndPlace-v3 / PandaStack-v3
  cl: 
    type: ['controldiscrete_const'] # nocl / nullcl / predefined / predefinedtwostage / predefinedthreestage / selfpaced / selfpaceddual / controldiscrete / controldiscreteadaptive
    range_growth_mode: ['simple']
####################################################################################################

process_1:
  agent: 
    type: ['sac','td3','ddpg']
  environment:
    reward:
      reward_shaping_type: ['sparse'] # sparse / state_change_bonus
      reward_bonus: [0.0]
  buffer:
    replay_buffer_size: [1e6]
    her:
      goal_selection_strategy: ['noher','final'] # noher / final / future / future_once / near / next
    highlights:
      mode: ['nohl'] # nohl / fix_th / ama
      batch_ratio: [0.0]
    per:
      mode: ['noper'] # noper / proportional
  trainer:
    total_timesteps: [5e5]
  eval:
    freq: [1e4]
  # structure is differend than in the config file ##################################################x
  task: 
    gympanda: ['PandaSlide-v3'] # PandaReach-v3 / PandaPush-v3 / PandaSlide-v3 / PandaPickAndPlace-v3 / PandaStack-v3
  cl: 
    type: ['controldiscrete_const'] # nocl / nullcl / predefined / predefinedtwostage / predefinedthreestage / selfpaced / selfpaceddual / controldiscrete / controldiscreteadaptive
    range_growth_mode: ['simple']
####################################################################################################

process_2:
  agent: 
    type: ['sac','td3','ddpg']
  environment:
    reward:
      reward_shaping_type: ['sparse'] # sparse / state_change_bonus
      reward_bonus: [0.0]
  buffer:
    replay_buffer_size: [1e6]
    her:
      goal_selection_strategy: ['noher','final'] # noher / final / future / future_once / near / next
    highlights:
      mode: ['nohl'] # nohl / fix_th / ama
      batch_ratio: [0.0]
    per:
      mode: ['noper'] # noper / proportional
  trainer:
    total_timesteps: [5e5]
  eval:
    freq: [1e4]
  # structure is differend than in the config file ##################################################x
  task: 
    gympanda: ['PandaPickAndPlace-v3'] # PandaReach-v3 / PandaPush-v3 / PandaSlide-v3 / PandaPickAndPlace-v3 / PandaStack-v3
  cl: 
    type: ['controldiscrete_const'] # nocl / nullcl / predefined / predefinedtwostage / predefinedthreestage / selfpaced / selfpaceddual / controldiscrete / controldiscreteadaptive
    range_growth_mode: ['simple']
####################################################################################################

process_3:
  agent: 
    type: ['sac','td3','ddpg']
  environment:
    reward:
      reward_shaping_type: ['sparse'] # sparse / state_change_bonus
      reward_bonus: [0.0]
  buffer:
    replay_buffer_size: [1e6]
    her:
      goal_selection_strategy: ['noher','final'] # noher / final / future / future_once / near / next
    highlights:
      mode: ['nohl'] # nohl / fix_th / ama
      batch_ratio: [0.0]
    per:
      mode: ['proportional'] # noper / proportional
  trainer:
    total_timesteps: [5e5]
  eval:
    freq: [1e4]
  # structure is differend than in the config file ##################################################x
  task: 
    gympanda: ['PandaPush-v3'] # PandaReach-v3 / PandaPush-v3 / PandaSlide-v3 / PandaPickAndPlace-v3 / PandaStack-v3
  cl: 
    type: ['controldiscrete_const'] # nocl / nullcl / predefined / predefinedtwostage / predefinedthreestage / selfpaced / selfpaceddual / controldiscrete / controldiscreteadaptive
    range_growth_mode: ['simple']
####################################################################################################

process_4:
  agent: 
    type: ['sac','td3','ddpg']
  environment:
    reward:
      reward_shaping_type: ['sparse'] # sparse / state_change_bonus
      reward_bonus: [0.0]
  buffer:
    replay_buffer_size: [1e6]
    her:
      goal_selection_strategy: ['noher','final'] # noher / final / future / future_once / near / next
    highlights:
      mode: ['nohl'] # nohl / fix_th / ama
      batch_ratio: [0.0]
    per:
      mode: ['proportional'] # noper / proportional
  trainer:
    total_timesteps: [5e5]
  eval:
    freq: [1e4]
  # structure is differend than in the config file ##################################################x
  task: 
    gympanda: ['PandaSlide-v3'] # PandaReach-v3 / PandaPush-v3 / PandaSlide-v3 / PandaPickAndPlace-v3 / PandaStack-v3
  cl: 
    type: ['controldiscrete_const'] # nocl / nullcl / predefined / predefinedtwostage / predefinedthreestage / selfpaced / selfpaceddual / controldiscrete / controldiscreteadaptive
    range_growth_mode: ['simple']
####################################################################################################

process_5:
  agent: 
    type: ['sac','td3','ddpg']
  environment:
    reward:
      reward_shaping_type: ['sparse'] # sparse / state_change_bonus
      reward_bonus: [0.0]
  buffer:
    replay_buffer_size: [1e6]
    her:
      goal_selection_strategy: ['noher','final'] # noher / final / future / future_once / near / next
    highlights:
      mode: ['nohl'] # nohl / fix_th / ama
      batch_ratio: [0.0]
    per:
      mode: ['proportional'] # noper / proportional
  trainer:
    total_timesteps: [5e5]
  eval:
    freq: [1e4]
  # structure is differend than in the config file ##################################################x
  task: 
    gympanda: ['PandaPickAndPlace-v3'] # PandaReach-v3 / PandaPush-v3 / PandaSlide-v3 / PandaPickAndPlace-v3 / PandaStack-v3
  cl: 
    type: ['controldiscrete_const'] # nocl / nullcl / predefined / predefinedtwostage / predefinedthreestage / selfpaced / selfpaceddual / controldiscrete / controldiscreteadaptive
    range_growth_mode: ['simple']
####################################################################################################